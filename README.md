# **DocuGenie** RAG System
<img src="https://github.com/user-attachments/assets/c187f445-3df9-4877-883f-4f07783c7d20" alt="image" width="300"/>


## Overview

This project provides a **Retrieval-Augmented Generation (RAG)** system that integrates internal documentation, external APIs, and language models to generate intelligent answers to user queries. This guide will walk you through the setup and installation process, including how to create a `.env` file, install dependencies, and configure the environment.

## Project Workflow

### 1. **User Input**

- The user submits a query through the web interface.
- The input could include:
  - API specifications files
  - Text queries
  - Markdown (`.md`) files
  - Code files

### 2. **Retrieval System (RAG Framework)**

**Retrieval-Augmented Generation (RAG) enhances response generation by retrieving relevant documents before generating an answer.**

- **FAISS Index (Vector Search)**:
  - Convert input documents into vector embeddings using a pre-trained embedding model (`sentence-transformers/all-MiniLM-L6-v2`).
  - Store document vectors in a `FAISS` (Facebook AI Similarity Search) index.
  - When a user submits a query, encode the query into an embedding.
  - Perform a similarity search in the FAISS index to find the most relevant documents.
  - Retrieve top-k relevant documents.

### 3. **Language Model (LLM) Processing**

- Load a pre-trained Language Model (e.g., LLama3-based model).
- Encode the retrieved documents and user query into tokenized format.
- Pass the tokenized data into the LLM as context.
- Generate a response based on both retrieved knowledge and the user query.
- Utilize a **Summary Model** `LLama-3` to process and store a summary of the conversation for historical tracking.
#### **Why Summarization Instead of Full Chat Storage?**
Storing the entire conversation can be inefficient due to:
- **Token Limit Constraints**: Exceeding token limits can hinder model performance.
- **Unnecessary Data Overload**: Retaining excess tokens may introduce irrelevant context.
- **Increased Processing Time**: Handling large conversations can slow down response generation.

Using a summary model optimizes storage, maintains relevant context, and ensures efficient processing.

### 4. **Response Generation**

- Format the generated response for readability.
- Ensure the response aligns with the retrieved documents for accuracy.
- Perform post-processing (e.g., filtering, summarization, or formatting).

### 5. **Output to User**

- Display the response in the web interface.
- Provide source references if applicable.
- Allow the user to refine their query if necessary.

### **How RAG Enhances the Process**

- **Contextual Retrieval**: Ensures the LLM has access to the most relevant documents.
- **Improved Accuracy**: The model generates responses based on both static knowledge and real-time retrieved data.
- **Scalability**: FAISS allows efficient large-scale document retrieval.
- **Dynamic Knowledge Updating**: Unlike static models, RAG enables real-time knowledge updates without retraining.
- **Conversation Summarization**: The Summary Model ensures an efficient historical record of the chat, improving continuity and context in future interactions.

By integrating FAISS, RAG, and a Summary Model, this system efficiently retrieves relevant information and enhances the accuracy of responses generated by the LLM while maintaining a historical chat summary.

## **Project Structure**

The project is organized into three main components: Backend, Frontend, and AI. Here's a breakdown of the structure:

### Backend (`backend/`)
- `routes/` - API route definitions
- `database/` - Database operations
- `faiss_index/` - FAISS index operations
- `vectorDB/` - Vector database operations
- `api_models/` - Database models and schemas

### Frontend (`frontend/`)
- `components/` - Reusable React components
- `assets/` - Static assets (images, styles)
- `styles/` - Global styles and themes
- `context/` - Frontend context
- `i18n/` - Internationalization configuration

### AI (`ai-core/`)
- `notebooks/` - Development notebooks for testing and experimentation
- `vectorDB/` - Vector database operations
- `phases/` - Implementation of phases


### Documentation
- `docs/` - Project documentation and diagrams
  - `drs/` - Design requirement specifications
  - `imgs/` - Diagram images
  - `api/` - API documentation

## Table of Contents

1. [Prerequisites](#prerequisites)
2. [Setting Up the Environment](#setting-up-the-environment)
3. [Installing Dependencies](#installing-dependencies)
4. [Running the Project](#running-the-project)
5. [Environment Variables](#environment-variables)
6. [Contributing](#contributing)
7. [Documents and Charts](#documents-and-charts)

---

### 1. Prerequisites

Before starting, ensure that you have the following software installed:

#### Backend Requirements
- Python 3.10 or higher
- `pip` (Python's package installer)
- Access to necessary API keys and models:
  - GROQ API Key
  - Sentence Transformers model
  - LLaMA model

#### Frontend Requirements
- Node.js (v18 or higher)
- npm (Node Package Manager)
- Modern web browser (Chrome, Firefox, Safari, or Edge)

#### Development Tools
- Git for version control
- Code editor (VS Code recommended)
- Terminal or command line interface

### 2. Setting Up the Environment

1. **Clone the repository**:

   First, clone the project repository to your local machine:

   ```bash
   git clone [https://github.com/mnarizzano/se24-p22](https://github.com/imblackline/DocuGenie-UI)
   cd DocuGenie-UI
   ```

### 2. Installing Dependencies

1. Install the required Frontend dependencies with npm:

   ```bash
   npm install
   ```

### 4. Running the Project

Once you have installed the dependencies, you can run the project by using the provided scripts or commands. Refer to the project-specific documentation for details on how to start the application. Example:

```bash
npm start
```

### 5. Contributing

We welcome contributions! If you want to contribute to the project, please fork the repository, create a new branch, make your changes, and then create a pull request. Make sure to follow our code guidelines and write unit tests where applicable.


---

### Contact

For any issues or questions regarding the project, feel free to reach out to the team via email. 

---
